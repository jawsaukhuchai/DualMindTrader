name: 🧪 DualMindTrader CI - Pytest & Coverage

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      # --- Checkout code from GitHub ---
      - name: 📦 Checkout repository
        uses: actions/checkout@v4

      # --- Set up Python environment ---
      - name: 🐍 Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      # --- Install dependencies ---
      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # --- Run tests with coverage ---
      - name: 🧪 Run pytest (skip benchmark)
        run: |
          pytest -m "not benchmark" --maxfail=1 --disable-warnings -q
          pytest -m "not benchmark" --cov=mind2_python --cov-report=term-missing --cov-report=xml --cov-report=html

      # --- Upload coverage results as artifact ---
      - name: 📊 Upload coverage HTML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-html
          path: htmlcov/

      - name: 📈 Upload coverage XML (for codecov or external analysis)
        uses: actions/upload-artifact@v4
        with:
          name: coverage-xml
          path: coverage.xml

  # Optional benchmark check (manual run only)
  benchmark:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest

    steps:
      - name: 📦 Checkout repository
        uses: actions/checkout@v4

      - name: 🐍 Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ⚡ Run performance benchmarks
        run: |
          pytest -m benchmark -s
          mkdir -p benchmarks/results
          python benchmarks/plot_benchmark_trend.py

      - name: 📤 Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmarks/results/
